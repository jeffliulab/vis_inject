import gradio as gr
import torch
from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration

# 1. åŠ è½½æ¨¡å‹ (åªéœ€åŠ è½½ä¸€æ¬¡)
print("æ­£åœ¨å¯åŠ¨ Web UIï¼Œè¯·ç¨å€™...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "Salesforce/blip2-opt-2.7b"

try:
    processor = Blip2Processor.from_pretrained(model_name)
    model = Blip2ForConditionalGeneration.from_pretrained(
        model_name, torch_dtype=torch.float16
    ).to(device)
    model.eval()
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    exit()

# 2. å®šä¹‰æ¨ç†å‡½æ•°
def predict(image):
    if image is None:
        return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡"
    
    # é¢„å¤„ç†
    inputs = processor(images=image, return_tensors="pt").to(device, torch.float16)
    pixel_values = inputs["pixel_values"]

    # æ‰‹åŠ¨æ¨ç†æµç¨‹ (å¤åˆ» inference.py çš„æˆåŠŸé€»è¾‘)
    with torch.no_grad():
        vision_outputs = model.vision_model(pixel_values=pixel_values)
        image_embeds = vision_outputs.last_hidden_state
        
        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=device)
        query_tokens = model.query_tokens.expand(image_embeds.shape[0], -1, -1)
        
        query_outputs = model.qformer(
            query_embeds=query_tokens,
            encoder_hidden_states=image_embeds,
            encoder_attention_mask=image_attention_mask,
        )
        query_output = query_outputs.last_hidden_state
        language_model_inputs = model.language_projection(query_output)
        
        inputs_embeds = language_model_inputs
        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=device)

        generated_ids = model.language_model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=50,
            do_sample=False
        )

    response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
    return response

# 3. æ­å»ºç•Œé¢
with gr.Blocks(title="BLIP-2 å¯¹æŠ—æ”»å‡»æ¼”ç¤º") as demo:
    gr.Markdown("# ğŸ›¡ï¸ VisInject å¯¹æŠ—æ”»å‡»æ¼”ç¤º")
    gr.Markdown("ä¸Šä¼ ä¸€å¼ å¯¹æŠ—æ ·æœ¬å›¾ç‰‡ï¼Œçœ‹çœ‹ BLIP-2 æ¨¡å‹ä¼šè¯´ä»€ä¹ˆã€‚")
    
    with gr.Row():
        with gr.Column():
            input_img = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡ (Drag & Drop)")
            run_btn = gr.Button("å¼€å§‹æ¨ç†", variant="primary")
        
        with gr.Column():
            output_text = gr.Textbox(label="æ¨¡å‹è¾“å‡º", lines=4, elem_id="output")
    
    # ç»‘å®šäº‹ä»¶
    run_btn.click(fn=predict, inputs=input_img, outputs=output_text)

# 4. å¯åŠ¨
if __name__ == "__main__":
    demo.launch(server_name="127.0.0.1", server_port=7860, share=False)
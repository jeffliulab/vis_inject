================================================================================
  VisInject — Ollama 外部测试失效原因分析 & 迁移攻击计划
================================================================================

一、问题现象
─────────────────────────────────────────────────────────────────
在 demo3_Qwen_2_5_VL_3B 中，对 kpop.png 的 PGD 攻击在内部测试中成功：
  - 验证-direct (tensor路径): 输出 "chicken dinner" ✓
  - 验证-png   (PNG路径):    输出 "chicken dinner" ✓
  - PSNR: 22.90 dB

但用 Ollama 客户端加载 qwen2.5-vl:3b 模型测试同一张对抗图像时，
输出与原图几乎无区别，对抗扰动完全失效。


二、根因分析
─────────────────────────────────────────────────────────────────

  【主因】模型权重量化

  ┌──────────────┬────────────────────────┬────────────────────────┐
  │              │ 我们的攻击环境           │ Ollama 运行环境          │
  ├──────────────┼────────────────────────┼────────────────────────┤
  │ 模型格式      │ HuggingFace safetensors │ GGUF                  │
  │ 权重精度      │ bfloat16 (16-bit)      │ Q4_K_M (约 4-bit)      │
  │ 参数值        │ 原始连续值              │ 量化后的离散近似值        │
  │ 模型大小      │ ~7 GB                  │ ~2-3 GB                │
  └──────────────┴────────────────────────┴────────────────────────┘

  PGD 攻击通过梯度 ∂loss/∂pixel 精确计算每个像素的最优扰动方向。
  这些梯度完全基于 bf16 权重计算，本质上是在 bf16 模型的损失曲面上寻找的
  对抗样本。

  当模型被量化到 4-bit 时：
  1. 权重值发生离散化跳变 (如 0.0137 → 0.0156)
     3B 参数的累积偏移彻底改变了模型的决策边界
  2. 攻击时计算的梯度方向在量化模型上不再有效
     同样的像素扰动在量化模型上产生的 loss 变化方向和幅度都不同
  3. 对抗样本的可迁移性已被学术界证实难以跨量化级别保持
     (参考: Quantization-aware adversarial attacks, ICLR/NeurIPS 相关论文)

  类比：精确到微米级的钥匙(扰动)配的是 bf16 的锁(模型)，
       Ollama 的锁芯(Q4量化)齿形不同，钥匙自然打不开。


  【次因】图像分辨率可能不一致

  我们的攻击固定使用 392×392 分辨率。
  Qwen2.5-VL 原生支持动态分辨率，Ollama 使用 smart_resize() 保持宽高比
  并对齐到 28 的倍数。

  - 保存的对抗 PNG 为 392×392，Ollama 理论上应保持该分辨率
  - 但若 Ollama 的 min_pixels/max_pixels 默认值不同，可能触发二次缩放
  - 任何缩放/插值都会破坏精心构造的像素级扰动结构

  该因素在确认 Ollama 保持 392×392 的情况下可排除，
  但作为迁移攻击的鲁棒性设计仍需考虑。


三、迁移攻击方案（按实现难度排序）
─────────────────────────────────────────────────────────────────

  方案 1：输入多样性攻击 (Input Diversity, DI-FGSM)
  ───────────────────────────────────────────────────
  难度: ★★☆☆☆   效果预期: 中等
  原理: 攻击时每步对图像施加随机变换(缩放、填充、JPEG压缩等)，
       迫使扰动不依赖特定的精确数值路径，从而提升泛化性。
  实现:
    - 在 pgd_attack.py 的每次迭代中，对 adv_image 施加随机变换后再计算 loss
    - 变换类型: 随机 resize (0.9x~1.1x) + padding + 随机 JPEG 压缩
    - 不需要修改模型加载，改动量最小
  参考: Xie et al., "Improving Transferability of Adversarial Examples
        with Input Diversity" (CVPR 2019)


  方案 2：动量迭代攻击 (MI-FGSM)
  ─────────────────────────────────
  难度: ★★☆☆☆   效果预期: 中等
  原理: 在梯度更新中加入动量项，稳定梯度方向，避免陷入
       bf16 模型特有的局部最优，找到更通用的扰动方向。
  实现:
    - 在 pgd_attack.py 中维护梯度动量 g_t = μ·g_{t-1} + grad/||grad||
    - 用 g_t 的符号方向更新扰动，而非直接用当前梯度
    - 通常 μ=1.0，与 DI-FGSM 可叠加使用
  参考: Dong et al., "Boosting Adversarial Attacks with Momentum"
        (CVPR 2018)


  方案 3：集成模型攻击 (Ensemble Attack)
  ───────────────────────────────────────
  难度: ★★★☆☆   效果预期: 高
  原理: 同时对 bf16 原始模型和量化模型计算 loss，取加权平均梯度，
       找到在两种精度下都有效的扰动。
  实现:
    - 用 GPTQ 或 AWQ 加载同一模型的 int4/int8 量化版本
    - 每步计算 loss = 0.5·loss_bf16 + 0.5·loss_q4
    - 需要显存能同时容纳两个模型 (~7+3=10GB)，或分批前向传播
  挑战:
    - 量化模型的梯度计算需要特殊处理 (STE 直通估计器)
    - GPTQ/AWQ 格式与 HuggingFace 兼容性较好


  方案 4：特征空间攻击 (Feature-level Attack)
  ────────────────────────────────────────────
  难度: ★★★☆☆   效果预期: 高
  原理: 不优化输出 token 的 CE loss，而是优化 vision encoder 输出的
       特征向量，使其偏向目标文本的嵌入方向。
       视觉编码器 (ViT) 通常不被量化或量化影响较小，
       因此特征级扰动更可能跨量化迁移。
  实现:
    - 计算 target_text 的文本嵌入 (通过 LLM 的 embedding 层)
    - 计算 image 经 ViT+PatchMerger 后的视觉嵌入
    - loss = -cosine_similarity(vision_embed, text_embed)
    - 反向传播到像素
  优势: 绕过 LLM 量化问题，直接在共享嵌入空间操作


  方案 5：直接攻击量化模型
  ─────────────────────────
  难度: ★★★★☆   效果预期: 最高（针对特定量化版本）
  原理: 加载与 Ollama 相同量化级别的模型，在量化权重上直接计算梯度。
  实现:
    - 使用 AutoGPTQ/AutoAWQ 加载 Q4 模型
    - 梯度通过 STE (Straight-Through Estimator) 近似量化操作
    - 或使用 llama.cpp 的 Python 绑定直接加载 GGUF
  挑战:
    - GGUF 格式不原生支持 PyTorch 梯度计算
    - 需要自行实现量化反向传播或使用代理模型
    - 仅针对特定量化版本有效，换版本需重新攻击


四、推荐实施路线
─────────────────────────────────────────────────────────────────

  阶段 1 (快速验证):
    → 方案 1 (输入多样性) + 方案 2 (动量迭代)
    改动量小，可在现有 pgd_attack.py 基础上快速实现
    两者可叠加，即 MI-DI-FGSM

  阶段 2 (效果提升):
    → 方案 4 (特征空间攻击)
    绕过 LLM 量化问题，直接在视觉-语言共享嵌入空间操作
    需重写 compute_attack_loss，但不需要额外模型

  阶段 3 (针对性攻击):
    → 方案 3 (集成攻击) 或 方案 5 (直接攻击量化模型)
    需要额外的模型加载，显存需求更高
    适合上 HPC 后实施


五、补充说明
─────────────────────────────────────────────────────────────────

  当前已修复的问题 (2026-02-18):
  - Labels 对齐 bug: apply_chat_template 在 <|im_end|> 后追加 \n，
    导致 label 定位偏移 1 个 token。"chicken dinner" 在 Qwen2.5 tokenizer
    中是 3 个 token (ch + icken + dinner)，错位导致第一个 token "ch"
    从未被训练，攻击无法在生成时产生 "chicken" 开头。
    修复: 通过定位最后一个 <|im_end|> 来精确确定 target 位置。

  内部测试已验证修复有效:
  - bf16 模型输出精确为 "chicken dinner" (full=True, kw=2/2)
  - Direct (tensor) 和 PNG 两条路径完全一致
  - PSNR 22.90 dB

================================================================================
